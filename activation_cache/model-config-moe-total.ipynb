{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining MoE checkpoint structure:\n",
      "MoE checkpoint keys from moe-total/best_val_loss_moe_step_9000.pt:\n",
      "  layers.0.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.0.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.0.attn_norm.w: torch.Size([768])\n",
      "  layers.0.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.0.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.0.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.0.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.0.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.0.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.1.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.1.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.1.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.1.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.1.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.2.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.2.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.2.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.2.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.2.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.3.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.3.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.3.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.3.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.3.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.4.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.4.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.4.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.4.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.4.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.5.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.5.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.5.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.5.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.5.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.6.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.6.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.6.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.6.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.6.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.7.out.weight: torch.Size([768, 384])\n",
      "  layers.0.ffn.experts.7.w_1.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.7.w_1.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.experts.7.w_2.bias: torch.Size([384])\n",
      "  layers.0.ffn.experts.7.w_2.weight: torch.Size([384, 768])\n",
      "  layers.0.ffn.router.bias: torch.Size([8])\n",
      "  layers.0.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.0.ffn_norm.w: torch.Size([768])\n",
      "  layers.1.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.1.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.1.attn_norm.w: torch.Size([768])\n",
      "  layers.1.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.0.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.0.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.0.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.0.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.0.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.1.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.1.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.1.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.1.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.1.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.2.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.2.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.2.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.2.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.2.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.3.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.3.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.3.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.3.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.3.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.4.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.4.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.4.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.4.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.4.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.5.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.5.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.5.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.5.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.5.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.6.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.6.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.6.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.6.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.6.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.7.out.weight: torch.Size([768, 384])\n",
      "  layers.1.ffn.experts.7.w_1.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.7.w_1.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.experts.7.w_2.bias: torch.Size([384])\n",
      "  layers.1.ffn.experts.7.w_2.weight: torch.Size([384, 768])\n",
      "  layers.1.ffn.router.bias: torch.Size([8])\n",
      "  layers.1.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.1.ffn_norm.w: torch.Size([768])\n",
      "  layers.2.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.2.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.2.attn_norm.w: torch.Size([768])\n",
      "  layers.2.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.0.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.0.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.0.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.0.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.0.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.1.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.1.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.1.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.1.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.1.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.2.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.2.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.2.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.2.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.2.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.3.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.3.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.3.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.3.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.3.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.4.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.4.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.4.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.4.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.4.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.5.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.5.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.5.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.5.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.5.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.6.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.6.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.6.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.6.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.6.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.7.out.weight: torch.Size([768, 384])\n",
      "  layers.2.ffn.experts.7.w_1.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.7.w_1.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.experts.7.w_2.bias: torch.Size([384])\n",
      "  layers.2.ffn.experts.7.w_2.weight: torch.Size([384, 768])\n",
      "  layers.2.ffn.router.bias: torch.Size([8])\n",
      "  layers.2.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.2.ffn_norm.w: torch.Size([768])\n",
      "  layers.3.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.3.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.3.attn_norm.w: torch.Size([768])\n",
      "  layers.3.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.0.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.0.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.0.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.0.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.0.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.1.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.1.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.1.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.1.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.1.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.2.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.2.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.2.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.2.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.2.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.3.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.3.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.3.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.3.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.3.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.4.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.4.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.4.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.4.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.4.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.5.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.5.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.5.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.5.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.5.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.6.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.6.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.6.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.6.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.6.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.7.out.weight: torch.Size([768, 384])\n",
      "  layers.3.ffn.experts.7.w_1.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.7.w_1.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.experts.7.w_2.bias: torch.Size([384])\n",
      "  layers.3.ffn.experts.7.w_2.weight: torch.Size([384, 768])\n",
      "  layers.3.ffn.router.bias: torch.Size([8])\n",
      "  layers.3.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.3.ffn_norm.w: torch.Size([768])\n",
      "  layers.4.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.4.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.4.attn_norm.w: torch.Size([768])\n",
      "  layers.4.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.0.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.0.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.0.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.0.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.0.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.1.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.1.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.1.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.1.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.1.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.2.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.2.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.2.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.2.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.2.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.3.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.3.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.3.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.3.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.3.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.4.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.4.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.4.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.4.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.4.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.5.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.5.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.5.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.5.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.5.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.6.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.6.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.6.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.6.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.6.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.7.out.weight: torch.Size([768, 384])\n",
      "  layers.4.ffn.experts.7.w_1.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.7.w_1.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.experts.7.w_2.bias: torch.Size([384])\n",
      "  layers.4.ffn.experts.7.w_2.weight: torch.Size([384, 768])\n",
      "  layers.4.ffn.router.bias: torch.Size([8])\n",
      "  layers.4.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.4.ffn_norm.w: torch.Size([768])\n",
      "  norm.w: torch.Size([768])\n",
      "  output.bias: torch.Size([50257])\n",
      "  output.weight: torch.Size([50257, 768])\n",
      "  tok_embedding.weight: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# 🔧 MoE Model Conversion - CORRECTED Configuration\n",
    "# \n",
    "# ✅ FIXED ISSUES after reading actual tiny-moe codebase:\n",
    "# 1. vocab_size: 50256 (was 50257) - from config.py\n",
    "# 2. d_hidden: 384 confirmed (d_model // 2 for TOTAL params) - from layer.py comment\n",
    "# 3. bos/eos token IDs: 50255 (vocab_size - 1)\n",
    "# 4. All parameters now match actual MoE training configuration\n",
    "\n",
    "# First, let's examine the MoE checkpoint structure\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load an MoE checkpoint to understand structure\n",
    "def examine_moe_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    print(f\"MoE checkpoint keys from {checkpoint_path}:\")\n",
    "    for k in sorted(state_dict.keys()):\n",
    "        print(f\"  {k}: {state_dict[k].shape}\")\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "# Examine a couple of MoE checkpoints\n",
    "print(\"Examining MoE checkpoint structure:\")\n",
    "moe_state = examine_moe_checkpoint(\"best_val_loss_moe_step_9000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for MoE model (based on actual tiny-moe codebase analysis)\n",
    "# From layer.py: d_hidden = d_model // 2 for \"total params\", d_hidden = d_model * 2 for \"active params\"\n",
    "# User wants total params with d_hidden = 384, where d_model = 768, so 384 = 768 // 2 ✓\n",
    "\n",
    "def create_moe_config():\n",
    "    \"\"\"\n",
    "    Create a GPT2-style config for the MoE model.\n",
    "    \n",
    "    Based on ACTUAL tiny-moe codebase analysis and conversion:\n",
    "    - d_model = 768 (from config.py)\n",
    "    - d_hidden = 384 (d_model // 2 for TOTAL params - from layer.py comment!)\n",
    "    - n_experts = 8, top_k = 2 (from config.py)\n",
    "    - vocab_size = 50257 (actual trained model)\n",
    "    \n",
    "    Key insight from layer.py line 28:\n",
    "    # for matching total params just do d_hidden = d_model // 2 & d_hidden = d_model * 2 for matching active params\n",
    "    \"\"\"\n",
    "    config_dict = {\n",
    "        \"model_type\": \"gpt2\",\n",
    "        \"vocab_size\": 50257,  # ✅ CORRECTED: actual trained model has 50257 (standard GPT-2)\n",
    "        \"n_positions\": 1024,  # max_seq_len from MoE config\n",
    "        \"n_embd\": 768,        # d_model from MoE config\n",
    "        \"n_layer\": 5,         # ✅ CRITICAL: MoE has exactly 5 layers (0-4)\n",
    "        \"n_head\": 12,         # n_heads from MoE config  \n",
    "        \"n_inner\": 384,       # ✅ CRITICAL: d_hidden = 384 for TOTAL params (NOT 3072!)\n",
    "        \"activation_function\": \"gelu_new\",\n",
    "        \"resid_pdrop\": 0.1,   # dropout from MoE config\n",
    "        \"embd_pdrop\": 0.1,    # dropout from MoE config\n",
    "        \"attn_pdrop\": 0.1,    # attn_dropout from MoE config\n",
    "        \"layer_norm_epsilon\": 1e-06,  # norm_eps from MoE config\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"bos_token_id\": 50256,  # vocab_size - 1 = 50257 - 1 = 50256\n",
    "        \"eos_token_id\": 50256,  # vocab_size - 1 = 50257 - 1 = 50256\n",
    "        \"architectures\": [\"GPT2LMHeadModel\"],\n",
    "        \"task_specific_params\": {\n",
    "            \"text-generation\": {\n",
    "                \"do_sample\": True,\n",
    "                \"max_length\": 50\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save config to both locations\n",
    "    import os\n",
    "    os.makedirs(\"moe-total-converted\", exist_ok=True)\n",
    "    \n",
    "    # Save to root (for local testing)\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    # Save to converted directory (for HuggingFace upload)\n",
    "    with open(\"moe-total-converted/config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "# Create the config\n",
    "config = create_moe_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated MoE conversion function created!\n"
     ]
    }
   ],
   "source": [
    "def convert_moe_checkpoint_to_hf(checkpoint_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert MoE checkpoint to HuggingFace GPT2 format.\n",
    "    \n",
    "    MoE Architecture (from codebase analysis):\n",
    "    - Each expert is SwiGLUFFN with w_1, w_2, out layers\n",
    "    - d_hidden = 384 (for total params), d_model = 768\n",
    "    - 8 experts, top_k = 2\n",
    "    \n",
    "    SwiGLU -> GPT2 MLP conversion strategy:\n",
    "    - Combine w_1 and w_2 weights (SwiGLU gates) -> c_fc\n",
    "    - Use out weights -> c_proj\n",
    "    \"\"\"\n",
    "    # Load MoE checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Map MoE model's keys to GPT2 keys\n",
    "    hf_state_dict = {}\n",
    "    \n",
    "    # Collect expert weights by layer for averaging\n",
    "    expert_weights = {}  # layer_num -> weight_type -> param_type -> expert_id -> tensor\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        # Token embeddings - use actual trained vocab size\n",
    "        if \"tok_embedding.weight\" in key:\n",
    "            hf_state_dict[\"transformer.wte.weight\"] = value\n",
    "            print(f\"✅ Token embeddings: {value.shape}\")\n",
    "            \n",
    "        # Output layer - use actual trained vocab size\n",
    "        elif \"output.weight\" in key:\n",
    "            hf_state_dict[\"lm_head.weight\"] = value\n",
    "            print(f\"✅ Output weights: {value.shape}\")\n",
    "            \n",
    "        # Output bias - Skip for standard GPT-2 compatibility\n",
    "        elif \"output.bias\" in key:\n",
    "            # Standard GPT-2 doesn't use lm_head.bias, so skip this\n",
    "            print(f\"⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\")\n",
    "            continue\n",
    "            \n",
    "        # Final layer norm (same as dense)\n",
    "        elif \"norm.w\" in key and \"layers\" not in key:\n",
    "            hf_state_dict[\"transformer.ln_f.weight\"] = value\n",
    "            \n",
    "        # Transformer layers\n",
    "        elif \"layers\" in key:\n",
    "            parts = key.split(\".\")\n",
    "            layer_num = int(parts[1])\n",
    "            \n",
    "            # Attention weights (same as dense conversion)\n",
    "            if \"attention.c_attn.weight\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.weight\"] = value.T\n",
    "            elif \"attention.c_proj.weight\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.weight\"] = value.T\n",
    "                \n",
    "            # Layer norms (same as dense)\n",
    "            elif \"attn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_1.weight\"] = value\n",
    "            elif \"ffn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_2.weight\"] = value\n",
    "                \n",
    "            # MoE expert weights - collect for averaging\n",
    "            elif \"ffn.experts\" in key:\n",
    "                # Parse: layers.{layer_num}.ffn.experts.{expert_id}.{weight_type}.{param_type}\n",
    "                expert_id = int(parts[4])  # expert number (0-7)\n",
    "                weight_type = parts[5]     # w_1, w_2, or out\n",
    "                param_type = parts[6]      # weight or bias\n",
    "                \n",
    "                # Initialize nested dict structure\n",
    "                if layer_num not in expert_weights:\n",
    "                    expert_weights[layer_num] = {}\n",
    "                if weight_type not in expert_weights[layer_num]:\n",
    "                    expert_weights[layer_num][weight_type] = {}\n",
    "                if param_type not in expert_weights[layer_num][weight_type]:\n",
    "                    expert_weights[layer_num][weight_type][param_type] = {}\n",
    "                    \n",
    "                expert_weights[layer_num][weight_type][param_type][expert_id] = value\n",
    "                \n",
    "            # Skip router weights (not needed for standard GPT2)\n",
    "            elif \"ffn.router\" in key:\n",
    "                continue  # Silent skip\n",
    "    \n",
    "    print(f\"Found expert weights for {len(expert_weights)} layers\")\n",
    "    \n",
    "    # Convert SwiGLU experts to standard GPT2 MLP for each layer\n",
    "    for layer_num in range(5):  # 5 layers\n",
    "        if layer_num in expert_weights:\n",
    "            layer_experts = expert_weights[layer_num]\n",
    "            \n",
    "            # SwiGLU has: out(w_1(x) * silu(w_2(x)))\n",
    "            # Convert to GPT2 MLP: c_proj(gelu(c_fc(x)))\n",
    "            \n",
    "            # Strategy: Average w_1 weights across experts for c_fc\n",
    "            # (w_2 is used for gating in SwiGLU, less critical for simple conversion)\n",
    "            if \"w_1\" in layer_experts and \"weight\" in layer_experts[\"w_1\"]:\n",
    "                w1_weights = torch.stack(list(layer_experts[\"w_1\"][\"weight\"].values()))\n",
    "                avg_w1_weight = w1_weights.mean(dim=0)  # Shape: (384, 768)\n",
    "                # Transpose for GPT2 format: (768, 384)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.weight\"] = avg_w1_weight.T\n",
    "                \n",
    "            if \"w_1\" in layer_experts and \"bias\" in layer_experts[\"w_1\"]:\n",
    "                w1_biases = torch.stack(list(layer_experts[\"w_1\"][\"bias\"].values()))\n",
    "                avg_w1_bias = w1_biases.mean(dim=0)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.bias\"] = avg_w1_bias\n",
    "            \n",
    "            # Use out weights for c_proj\n",
    "            if \"out\" in layer_experts and \"weight\" in layer_experts[\"out\"]:\n",
    "                out_weights = torch.stack(list(layer_experts[\"out\"][\"weight\"].values()))\n",
    "                avg_out_weight = out_weights.mean(dim=0)  # Shape: (768, 384)\n",
    "                # Transpose for GPT2 format: (384, 768)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.weight\"] = avg_out_weight.T\n",
    "                \n",
    "            if \"out\" in layer_experts and \"bias\" in layer_experts[\"out\"]:\n",
    "                out_biases = torch.stack(list(layer_experts[\"out\"][\"bias\"].values()))\n",
    "                avg_out_bias = out_biases.mean(dim=0)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.bias\"] = avg_out_bias\n",
    "                \n",
    "        else:\n",
    "            print(f\"Warning: No expert weights found for layer {layer_num}\")\n",
    "    \n",
    "    # Add missing components that GPT2 expects but MoE model doesn't have\n",
    "    # 1. Positional embeddings (MoE uses RoPE, initialize to zeros)\n",
    "    hf_state_dict[\"transformer.wpe.weight\"] = torch.zeros(1024, 768)\n",
    "    \n",
    "    # 2. Layer norm bias terms (MoE uses RMSNorm without bias)\n",
    "    hf_state_dict[\"transformer.ln_f.bias\"] = torch.zeros(768)\n",
    "    for layer_num in range(5):\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_1.bias\"] = torch.zeros(768)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_2.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # 3. Attention bias terms (MoE doesn't have these)\n",
    "    for layer_num in range(5):\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.bias\"] = torch.zeros(2304)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # Save as pytorch_model.bin\n",
    "    torch.save(hf_state_dict, output_path)\n",
    "    print(f\"✅ Converted MoE checkpoint: {checkpoint_path} -> {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MoE TOTAL test functions created!\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Test the converted MoE model by loading it and running a simple generation.\"\"\"\n",
    "# from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "# import torch\n",
    "# def test_converted_moe_activated_model(weight_path, test_input):\n",
    "#     # Load config from current directory\n",
    "#     config = GPT2Config.from_pretrained(\"./\")\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = GPT2LMHeadModel(config)\n",
    "\n",
    "#     # Load converted weights\n",
    "#     state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "\n",
    "#     # Try loading the state dict\n",
    "#     try:\n",
    "#         missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "#         print(f\"\\nLoading report for {weight_path}:\")\n",
    "#         print(f\"Missing keys: {len(missing_keys)}\")\n",
    "#         if missing_keys:\n",
    "#             print(\"  \", missing_keys[:5], \"...\" if len(missing_keys) > 5 else \"\")\n",
    "            \n",
    "#         print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
    "#         if unexpected_keys:\n",
    "#             print(\"  \", unexpected_keys[:5], \"...\" if len(unexpected_keys) > 5 else \"\")\n",
    "            \n",
    "#         if len(missing_keys) == 0 and len(unexpected_keys) == 0:\n",
    "#             print(\"✅ Perfect MoE conversion!\")\n",
    "            \n",
    "#             # Test the model with a simple generation\n",
    "#             model.eval()\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "#             # Test generation\n",
    "#             inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(inputs[\"input_ids\"], max_length=30, do_sample=True, temperature=0.7)\n",
    "            \n",
    "#             generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#             print(f\"\\nTest generation:\")\n",
    "#             print(f\"Input: {test_input}\")\n",
    "#             print(f\"Output: {generated_text}\")\n",
    "            \n",
    "#         else:\n",
    "#             print(\"⚠️  MoE conversion completed with some mismatches\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error during loading: {e}\")\n",
    "#         pass\n",
    "\n",
    "\n",
    "def test_converted_moe_total_model(weight_path, test_input):\n",
    "    \"\"\"Test the converted MoE TOTAL model by loading it and running a simple generation.\"\"\"\n",
    "    from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "    import torch\n",
    "\n",
    "    # Load config from current directory (should be the total config with n_inner=384)\n",
    "    config = GPT2Config.from_pretrained(\"./\")\n",
    "    \n",
    "    print(f\"Testing with config: n_inner={config.n_inner} (should be 384 for total)\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Load converted weights\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "\n",
    "    # Try loading the state dict\n",
    "    try:\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        print(f\"\\n✅ Loading report for {weight_path}:\")\n",
    "        print(f\"Missing keys: {len(missing_keys)}\")\n",
    "        if missing_keys:\n",
    "            print(\"  \", missing_keys[:5], \"...\" if len(missing_keys) > 5 else \"\")\n",
    "            \n",
    "        print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
    "        if unexpected_keys:\n",
    "            print(\"  \", unexpected_keys[:5], \"...\" if len(unexpected_keys) > 5 else \"\")\n",
    "            \n",
    "        if len(missing_keys) == 0 and len(unexpected_keys) == 0:\n",
    "            print(\"🎉 Perfect MoE TOTAL conversion!\")\n",
    "            \n",
    "            # Test the model with a simple generation\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "            # Test generation\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs[\"input_ids\"], \n",
    "                    max_length=30, \n",
    "                    do_sample=True, \n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\n🎯 Test generation:\")\n",
    "            print(f\"Input: {test_input}\")\n",
    "            print(f\"Output: {generated_text}\")\n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️  MoE TOTAL conversion completed with some mismatches\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during loading: {e}\")\n",
    "        return False\n",
    "\n",
    "# Also create a general function that can be called as test_converted_moe_model\n",
    "def test_converted_moe_model(weight_path, test_input):\n",
    "    \"\"\"Alias for test_converted_moe_total_model\"\"\"\n",
    "    return test_converted_moe_total_model(weight_path, test_input)\n",
    "\n",
    "print(\"✅ MoE TOTAL test functions created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Testing FINAL conversion: moe-total/best_val_loss_moe_step_9000.pt...\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_9000.pt -> converted_moe_test_final.bin\n",
      "\n",
      "🧪 Testing final corrected model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading report for converted_moe_test_final.bin:\n",
      "Missing keys: 0\n",
      "Unexpected keys: 0\n",
      "✅ Perfect MoE conversion!\n",
      "\n",
      "Test generation:\n",
      "Input: Once upon a time\n",
      "Output: Once upon a time Alex deeds Club sent Mia shiny keys shiny Mia Mia keys Mia Mia Mia Mia Mia Mia Mia Mia hate Mia Mia Mia Mia Mia Mia\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test the conversion with a single MoE checkpoint\n",
    "test_checkpoint = \"best_val_loss_moe_step_9000.pt\"\n",
    "test_output = \"converted_moe_test.bin\"\n",
    "\n",
    "print(f\"Converting {test_checkpoint}...\")\n",
    "convert_moe_checkpoint_to_hf(test_checkpoint, test_output)\n",
    "\n",
    "print(f\"\\nTesting converted model...\")\n",
    "test_converted_moe_model(test_output, \"Once upon a time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 MoE checkpoint files in moe-total/:\n",
      "  - best_val_loss_moe_step_900.pt\n",
      "  - best_val_loss_moe_step_6000.pt\n",
      "  - best_val_loss_moe_step_2100.pt\n",
      "  - moe_step_9000.pt\n",
      "  - best_val_loss_moe_step_5700.pt\n",
      "  - best_val_loss_moe_step_3600.pt\n",
      "  - best_val_loss_moe_step_300.pt\n",
      "  - best_val_loss_moe_step_6600.pt\n",
      "  - best_val_loss_moe_step_2700.pt\n",
      "  - best_val_loss_moe_step_3000.pt\n",
      "  - best_val_loss_moe_step_8400.pt\n",
      "  - moe_step_8000.pt\n",
      "  - best_val_loss_moe_step_5100.pt\n",
      "  - best_val_loss_moe_step_8700.pt\n",
      "  - moe_step_4000.pt\n",
      "  - best_val_loss_moe_step_7200.pt\n",
      "  - best_val_loss_moe_step_3300.pt\n",
      "  - best_val_loss_moe_step_4800.pt\n",
      "  - moe_step_6000.pt\n",
      "  - moe_step_2000.pt\n",
      "  - best_val_loss_moe_step_2400.pt\n",
      "  - best_val_loss_moe_step_1200.pt\n",
      "  - best_val_loss_moe_step_4500.pt\n",
      "  - best_val_loss_moe_step_6900.pt\n",
      "  - best_val_loss_moe_step_9000.pt\n",
      "  - moe_step_7000.pt\n",
      "  - moe_step_5000.pt\n",
      "  - best_val_loss_moe_step_7800.pt\n",
      "  - best_val_loss_moe_step_8100.pt\n",
      "  - best_val_loss_moe_step_3900.pt\n",
      "  - last_epoch_moe_0.pt\n",
      "  - best_val_loss_moe_step_4200.pt\n",
      "  - best_val_loss_moe_step_1500.pt\n",
      "  - best_val_loss_moe_step_5400.pt\n",
      "  - best_val_loss_moe_step_600.pt\n",
      "  - last_epoch_moe_1.pt\n",
      "  - moe_step_1000.pt\n",
      "  - best_val_loss_moe_step_1800.pt\n",
      "  - moe_step_3000.pt\n",
      "  - best_val_loss_moe_step_6300.pt\n",
      "  - best_val_loss_moe_step_7500.pt\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_900.pt -> moe-total-converted-test/best_val_loss_moe_step_900.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_900.pt -> moe-total-converted-test/best_val_loss_moe_step_900.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_6000.pt -> moe-total-converted-test/best_val_loss_moe_step_6000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_6000.pt -> moe-total-converted-test/best_val_loss_moe_step_6000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_2100.pt -> moe-total-converted-test/best_val_loss_moe_step_2100.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_2100.pt -> moe-total-converted-test/best_val_loss_moe_step_2100.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_9000.pt -> moe-total-converted-test/moe_step_9000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_9000.pt -> moe-total-converted-test/moe_step_9000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_5700.pt -> moe-total-converted-test/best_val_loss_moe_step_5700.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_5700.pt -> moe-total-converted-test/best_val_loss_moe_step_5700.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_3600.pt -> moe-total-converted-test/best_val_loss_moe_step_3600.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_3600.pt -> moe-total-converted-test/best_val_loss_moe_step_3600.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_300.pt -> moe-total-converted-test/best_val_loss_moe_step_300.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_300.pt -> moe-total-converted-test/best_val_loss_moe_step_300.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_6600.pt -> moe-total-converted-test/best_val_loss_moe_step_6600.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_6600.pt -> moe-total-converted-test/best_val_loss_moe_step_6600.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_2700.pt -> moe-total-converted-test/best_val_loss_moe_step_2700.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_2700.pt -> moe-total-converted-test/best_val_loss_moe_step_2700.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_3000.pt -> moe-total-converted-test/best_val_loss_moe_step_3000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_3000.pt -> moe-total-converted-test/best_val_loss_moe_step_3000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_8400.pt -> moe-total-converted-test/best_val_loss_moe_step_8400.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_8400.pt -> moe-total-converted-test/best_val_loss_moe_step_8400.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_8000.pt -> moe-total-converted-test/moe_step_8000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_8000.pt -> moe-total-converted-test/moe_step_8000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_5100.pt -> moe-total-converted-test/best_val_loss_moe_step_5100.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_5100.pt -> moe-total-converted-test/best_val_loss_moe_step_5100.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_8700.pt -> moe-total-converted-test/best_val_loss_moe_step_8700.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_8700.pt -> moe-total-converted-test/best_val_loss_moe_step_8700.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_4000.pt -> moe-total-converted-test/moe_step_4000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_4000.pt -> moe-total-converted-test/moe_step_4000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_7200.pt -> moe-total-converted-test/best_val_loss_moe_step_7200.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_7200.pt -> moe-total-converted-test/best_val_loss_moe_step_7200.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_3300.pt -> moe-total-converted-test/best_val_loss_moe_step_3300.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_3300.pt -> moe-total-converted-test/best_val_loss_moe_step_3300.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_4800.pt -> moe-total-converted-test/best_val_loss_moe_step_4800.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_4800.pt -> moe-total-converted-test/best_val_loss_moe_step_4800.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_6000.pt -> moe-total-converted-test/moe_step_6000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_6000.pt -> moe-total-converted-test/moe_step_6000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_2000.pt -> moe-total-converted-test/moe_step_2000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_2000.pt -> moe-total-converted-test/moe_step_2000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_2400.pt -> moe-total-converted-test/best_val_loss_moe_step_2400.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_2400.pt -> moe-total-converted-test/best_val_loss_moe_step_2400.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_1200.pt -> moe-total-converted-test/best_val_loss_moe_step_1200.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_1200.pt -> moe-total-converted-test/best_val_loss_moe_step_1200.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_4500.pt -> moe-total-converted-test/best_val_loss_moe_step_4500.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_4500.pt -> moe-total-converted-test/best_val_loss_moe_step_4500.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_6900.pt -> moe-total-converted-test/best_val_loss_moe_step_6900.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_6900.pt -> moe-total-converted-test/best_val_loss_moe_step_6900.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_9000.pt -> moe-total-converted-test/best_val_loss_moe_step_9000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_9000.pt -> moe-total-converted-test/best_val_loss_moe_step_9000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_7000.pt -> moe-total-converted-test/moe_step_7000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_7000.pt -> moe-total-converted-test/moe_step_7000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_5000.pt -> moe-total-converted-test/moe_step_5000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_5000.pt -> moe-total-converted-test/moe_step_5000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_7800.pt -> moe-total-converted-test/best_val_loss_moe_step_7800.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_7800.pt -> moe-total-converted-test/best_val_loss_moe_step_7800.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_8100.pt -> moe-total-converted-test/best_val_loss_moe_step_8100.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_8100.pt -> moe-total-converted-test/best_val_loss_moe_step_8100.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_3900.pt -> moe-total-converted-test/best_val_loss_moe_step_3900.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_3900.pt -> moe-total-converted-test/best_val_loss_moe_step_3900.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: last_epoch_moe_0.pt -> moe-total-converted-test/last_epoch_moe_0.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/last_epoch_moe_0.pt -> moe-total-converted-test/last_epoch_moe_0.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_4200.pt -> moe-total-converted-test/best_val_loss_moe_step_4200.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_4200.pt -> moe-total-converted-test/best_val_loss_moe_step_4200.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_1500.pt -> moe-total-converted-test/best_val_loss_moe_step_1500.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_1500.pt -> moe-total-converted-test/best_val_loss_moe_step_1500.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_5400.pt -> moe-total-converted-test/best_val_loss_moe_step_5400.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_5400.pt -> moe-total-converted-test/best_val_loss_moe_step_5400.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_600.pt -> moe-total-converted-test/best_val_loss_moe_step_600.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_600.pt -> moe-total-converted-test/best_val_loss_moe_step_600.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: last_epoch_moe_1.pt -> moe-total-converted-test/last_epoch_moe_1.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/last_epoch_moe_1.pt -> moe-total-converted-test/last_epoch_moe_1.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_1000.pt -> moe-total-converted-test/moe_step_1000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_1000.pt -> moe-total-converted-test/moe_step_1000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_1800.pt -> moe-total-converted-test/best_val_loss_moe_step_1800.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_1800.pt -> moe-total-converted-test/best_val_loss_moe_step_1800.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: moe_step_3000.pt -> moe-total-converted-test/moe_step_3000.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/moe_step_3000.pt -> moe-total-converted-test/moe_step_3000.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_6300.pt -> moe-total-converted-test/best_val_loss_moe_step_6300.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_6300.pt -> moe-total-converted-test/best_val_loss_moe_step_6300.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_moe_step_7500.pt -> moe-total-converted-test/best_val_loss_moe_step_7500.bin\n",
      "==================================================\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE checkpoint: moe-total/best_val_loss_moe_step_7500.pt -> moe-total-converted-test/best_val_loss_moe_step_7500.bin\n",
      "✅ Conversion successful\n",
      "\n",
      "🎉 All MoE conversions completed!\n"
     ]
    }
   ],
   "source": [
    "# Convert all MoE checkpoints (similar to dense conversion)\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Create output directory for converted MoE models\n",
    "output_dir = \"moe-total-converted-test/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all .pt files in the moe-total/ directory\n",
    "moe_dir = \"moe-total/\"\n",
    "if os.path.exists(moe_dir):\n",
    "    pt_files = glob.glob(os.path.join(moe_dir, \"*.pt\"))\n",
    "    \n",
    "    if pt_files:\n",
    "        print(f\"Found {len(pt_files)} MoE checkpoint files in {moe_dir}:\")\n",
    "        for pt_file in pt_files:\n",
    "            print(f\"  - {os.path.basename(pt_file)}\")\n",
    "        \n",
    "        # Convert each checkpoint\n",
    "        for pt_file in pt_files:\n",
    "            base_name = os.path.splitext(os.path.basename(pt_file))[0]\n",
    "            output_name = os.path.join(output_dir, f\"{base_name}.bin\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Converting: {os.path.basename(pt_file)} -> {output_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                convert_moe_checkpoint_to_hf(pt_file, output_name)\n",
    "                print(\"✅ Conversion successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Conversion failed: {e}\")\n",
    "    else:\n",
    "        print(f\"No .pt files found in {moe_dir}\")\n",
    "else:\n",
    "    print(f\"Directory {moe_dir} does not exist\")\n",
    "    # Fallback to root directory MoE files\n",
    "    root_moe_files = glob.glob(\"best_val_loss_moe_step_*.pt\")\n",
    "    if root_moe_files:\n",
    "        print(f\"Found {len(root_moe_files)} MoE files in root directory\")\n",
    "        for pt_file in root_moe_files:\n",
    "            base_name = os.path.splitext(os.path.basename(pt_file))[0]\n",
    "            output_name = os.path.join(output_dir, f\"{base_name}.bin\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Converting: {pt_file} -> {output_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                convert_moe_checkpoint_to_hf(pt_file, output_name)\n",
    "                print(\"✅ Conversion successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Conversion failed: {e}\")\n",
    "\n",
    "print(\"\\n🎉 All MoE conversions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GPT-2 tokenizer files for MoE model - FIXED VERSION\n",
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Saving GPT-2 tokenizer files for MoE model...\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"Standard GPT-2 tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"✅ MoE model vocab size: 50257 (matches actual trained model)\")\n",
    "\n",
    "# Save all tokenizer files to output directory\n",
    "tokenizer.save_pretrained(\"./moe-total-converted/\")\n",
    "\n",
    "# Save special_tokens_map.json - simplified approach without problematic `with`\n",
    "special_tokens_map = {\n",
    "    \"bos_token\": \"<|endoftext|>\",\n",
    "    \"eos_token\": \"<|endoftext|>\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"pad_token\": \"<|endoftext|>\"\n",
    "}\n",
    "\n",
    "# Simple file writing approach\n",
    "import json\n",
    "json_file_path = \"moe-total-converted/special_tokens_map.json\"\n",
    "json_data = json.dumps(special_tokens_map, indent=2)\n",
    "\n",
    "# Write the file simply\n",
    "f = open(json_file_path, 'w', encoding='utf-8')\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "print(\"✅ Tokenizer files saved:\")\n",
    "print(\"  - vocab.json\")\n",
    "print(\"  - merges.txt\") \n",
    "print(\"  - tokenizer_config.json\")\n",
    "print(\"  - special_tokens_map.json\")\n",
    "print(\"  - config.json\")\n",
    "\n",
    "print(f\"\\n✅ Vocab size: {len(tokenizer)}\")\n",
    "print(\"   Configuration matches actual MoE training\")\n",
    "\n",
    "print(\"\\n🎉 MoE model conversion setup complete!\")\n",
    "print(\"✅ All files ready for HuggingFace upload!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CORRECTED config and conversion\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def test_corrected_config_and_conversion():\n",
    "    \"\"\"Test the corrected config creates the right model structure\"\"\"\n",
    "    \n",
    "    print(\"🧪 Testing CORRECTED config and conversion...\")\n",
    "    \n",
    "    # Load the corrected config\n",
    "    config = GPT2Config.from_pretrained(\"./\")\n",
    "    print(f\"Config loaded: n_layer={config.n_layer}, n_inner={config.n_inner}\")\n",
    "    \n",
    "    # Create model from config\n",
    "    model = GPT2LMHeadModel(config)\n",
    "    print(f\"Model created with {config.n_layer} layers\")\n",
    "    \n",
    "    # Test conversion with a single MoE checkpoint\n",
    "    checkpoint_path = \"best_val_loss_moe_step_9000.pt\"\n",
    "    output_path = \"test_corrected_conversion.bin\"\n",
    "    \n",
    "    print(f\"\\n🔄 Converting {checkpoint_path} with corrected function...\")\n",
    "    convert_moe_checkpoint_to_hf(checkpoint_path, output_path)\n",
    "    \n",
    "    # Try loading the converted weights\n",
    "    print(f\"\\n✅ Testing weight loading...\")\n",
    "    state_dict = torch.load(output_path, map_location=\"cpu\")\n",
    "    \n",
    "    try:\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        print(f\"\\n📊 Loading report:\")\n",
    "        print(f\"Missing keys: {len(missing_keys)}\")\n",
    "        if missing_keys and len(missing_keys) < 10:\n",
    "            for key in missing_keys:\n",
    "                print(f\"  - {key}\")\n",
    "        elif missing_keys:\n",
    "            print(f\"  - First 5: {missing_keys[:5]}\")\n",
    "            \n",
    "        print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
    "        if unexpected_keys and len(unexpected_keys) < 10:\n",
    "            for key in unexpected_keys:\n",
    "                print(f\"  - {key}\")\n",
    "        elif unexpected_keys:\n",
    "            print(f\"  - First 5: {unexpected_keys[:5]}\")\n",
    "            \n",
    "        if len(missing_keys) == 0 and len(unexpected_keys) == 0:\n",
    "            \n",
    "            # Quick generation test\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "            test_input = \"The quick brown fox\"\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs[\"input_ids\"], \n",
    "                    max_length=30, \n",
    "                    do_sample=True, \n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\n🎯 Test generation:\")\n",
    "            print(f\"Input: {test_input}\")\n",
    "            print(f\"Output: {generated_text}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"⚠️  Still have some mismatches, but may work with strict=False\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during weight loading: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "success = test_corrected_config_and_conversion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Config\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def load_moe_model(checkpoint_name=\"best_val_loss_moe_step_9000.bin\", model_id=\"idhant297/moe-5l-total-test\"):\n",
    "    \"\"\"\n",
    "    Load a MoE model from HuggingFace Hub with a specific checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_name (str): The checkpoint filename to load\n",
    "        model_id (str): The HuggingFace model repository ID\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) loaded from the checkpoint\n",
    "    \"\"\"\n",
    "    print(f\"Loading MoE model from {model_id} checkpoint {checkpoint_name}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    config = GPT2Config.from_pretrained(model_id)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=checkpoint_name\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ MoE model loaded successfully from checkpoint {checkpoint_name}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text_moe(model, tokenizer, prompt, max_length=100, temperature=0.8, top_p=0.95, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using the loaded MoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded MoE model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        prompt (str): Input text prompt\n",
    "        max_length (int): Maximum length of generated text\n",
    "        temperature (float): Sampling temperature\n",
    "        top_p (float): Top-p sampling parameter\n",
    "        num_return_sequences (int): Number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "        list: Generated text sequences\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage for MoE model\n",
    "checkpoint_name = \"best_val_loss_moe_step_8400.bin\"\n",
    "model, tokenizer = load_moe_model(checkpoint_name)\n",
    "\n",
    "prompt = \"test test test\"\n",
    "\n",
    "generated = generate_text_moe(model, tokenizer, prompt, max_length=50)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
