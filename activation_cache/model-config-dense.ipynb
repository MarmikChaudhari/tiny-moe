{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"idhant297/dense-5l-arxiv_code_simplestories\")\n",
    "\n",
    "# config = GPT2Config.from_pretrained(\"idhant297/dense-5l-arxiv_code_simplestories\")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# First, load the config\n",
    "config = GPT2Config.from_pretrained(\"idhant297/dense-5l-arxiv_code_simplestories\")\n",
    "\n",
    "# Then load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"idhant297/dense-5l-arxiv_code_simplestories\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original checkpoint keys:\n",
      "  tok_embedding.weight: torch.Size([50257, 768])\n",
      "  layers.0.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.0.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.0.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.0.ffn.w_1.bias: torch.Size([3072])\n",
      "  layers.0.ffn.w_2.weight: torch.Size([3072, 768])\n",
      "  layers.0.ffn.w_2.bias: torch.Size([3072])\n",
      "  layers.0.ffn.out.weight: torch.Size([768, 3072])\n",
      "  layers.0.ffn.out.bias: torch.Size([768])\n",
      "  layers.0.attn_norm.w: torch.Size([768])\n",
      "  layers.0.ffn_norm.w: torch.Size([768])\n",
      "  layers.1.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.1.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.1.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.1.ffn.w_1.bias: torch.Size([3072])\n",
      "  layers.1.ffn.w_2.weight: torch.Size([3072, 768])\n",
      "  layers.1.ffn.w_2.bias: torch.Size([3072])\n",
      "  layers.1.ffn.out.weight: torch.Size([768, 3072])\n",
      "  layers.1.ffn.out.bias: torch.Size([768])\n",
      "  layers.1.attn_norm.w: torch.Size([768])\n",
      "  layers.1.ffn_norm.w: torch.Size([768])\n",
      "  layers.2.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.2.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.2.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.2.ffn.w_1.bias: torch.Size([3072])\n",
      "  layers.2.ffn.w_2.weight: torch.Size([3072, 768])\n",
      "  layers.2.ffn.w_2.bias: torch.Size([3072])\n",
      "  layers.2.ffn.out.weight: torch.Size([768, 3072])\n",
      "  layers.2.ffn.out.bias: torch.Size([768])\n",
      "  layers.2.attn_norm.w: torch.Size([768])\n",
      "  layers.2.ffn_norm.w: torch.Size([768])\n",
      "  layers.3.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.3.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.3.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.3.ffn.w_1.bias: torch.Size([3072])\n",
      "  layers.3.ffn.w_2.weight: torch.Size([3072, 768])\n",
      "  layers.3.ffn.w_2.bias: torch.Size([3072])\n",
      "  layers.3.ffn.out.weight: torch.Size([768, 3072])\n",
      "  layers.3.ffn.out.bias: torch.Size([768])\n",
      "  layers.3.attn_norm.w: torch.Size([768])\n",
      "  layers.3.ffn_norm.w: torch.Size([768])\n",
      "  layers.4.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.4.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.4.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.4.ffn.w_1.bias: torch.Size([3072])\n",
      "  layers.4.ffn.w_2.weight: torch.Size([3072, 768])\n",
      "  layers.4.ffn.w_2.bias: torch.Size([3072])\n",
      "  layers.4.ffn.out.weight: torch.Size([768, 3072])\n",
      "  layers.4.ffn.out.bias: torch.Size([768])\n",
      "  layers.4.attn_norm.w: torch.Size([768])\n",
      "  layers.4.ffn_norm.w: torch.Size([768])\n",
      "  norm.w: torch.Size([768])\n",
      "  output.weight: torch.Size([50257, 768])\n",
      "  output.bias: torch.Size([50257])\n",
      "\n",
      "First few parameters:\n",
      "  tok_embedding.weight: torch.Size([50257, 768])\n",
      "  layers.0.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.0.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.0.ffn.w_1.weight: torch.Size([3072, 768])\n",
      "  layers.0.ffn.w_1.bias: torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the original checkpoint structure\n",
    "import torch\n",
    "\n",
    "# Load the original checkpoint\n",
    "checkpoint = torch.load(\"best_val_loss_dense_step_9000.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(\"Original checkpoint keys:\")\n",
    "for k in checkpoint[\"model_state_dict\"].keys():\n",
    "    print(f\"  {k}: {checkpoint['model_state_dict'][k].shape}\")\n",
    "\n",
    "print(\"\\nFirst few parameters:\")\n",
    "for i, (k, v) in enumerate(checkpoint[\"model_state_dict\"].items()):\n",
    "    if i < 5:\n",
    "        print(f\"  {k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed conversion function based on actual checkpoint structure\n",
    "import json\n",
    "import torch\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# 1. Create config.json - Update vocab_size to match actual model (50257 instead of 50256)\n",
    "config_dict = {\n",
    "    \"model_type\": \"gpt2\",\n",
    "    \"vocab_size\": 50257,  # Updated to match actual model\n",
    "    \"n_positions\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 5,\n",
    "    \"n_head\": 12,\n",
    "    \"n_inner\": 3072,  # Usually 4 * n_embd for GPT-2\n",
    "    \"activation_function\": \"gelu_new\",\n",
    "    \"resid_pdrop\": 0.1,\n",
    "    \"embd_pdrop\": 0.1,\n",
    "    \"attn_pdrop\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-06,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"bos_token_id\": 50256,\n",
    "    \"eos_token_id\": 50256,\n",
    "    \"architectures\": [\"GPT2LMHeadModel\"],\n",
    "    \"task_specific_params\": {\n",
    "        \"text-generation\": {\n",
    "            \"do_sample\": True,\n",
    "            \"max_length\": 50\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save updated config\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# 2. Convert your checkpoint to HuggingFace format with correct weight mapping\n",
    "def convert_checkpoint_to_hf(checkpoint_path, output_path):\n",
    "    # Load your checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Map your model's keys to GPT2 keys\n",
    "    hf_state_dict = {}\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        # Token embeddings\n",
    "        if \"tok_embedding.weight\" in key:\n",
    "            hf_state_dict[\"transformer.wte.weight\"] = value\n",
    "        # Output layer\n",
    "        elif \"output.weight\" in key:\n",
    "            hf_state_dict[\"lm_head.weight\"] = value\n",
    "        # Final layer norm - note: your model uses RMSNorm with 'w' parameter\n",
    "        elif \"norm.w\" in key and \"layers\" not in key:\n",
    "            hf_state_dict[\"transformer.ln_f.weight\"] = value\n",
    "        # Transformer layers\n",
    "        elif \"layers\" in key:\n",
    "            # Parse layer number\n",
    "            layer_num = int(key.split(\".\")[1])\n",
    "            \n",
    "            # Attention weights - need to transpose for GPT2 format\n",
    "            if \"attention.c_attn.weight\" in key:\n",
    "                # Your model: (2304, 768) -> GPT2: (768, 2304)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.weight\"] = value.T\n",
    "            elif \"attention.c_proj.weight\" in key:\n",
    "                # Your model: (768, 768) -> GPT2: (768, 768) - need to transpose\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.weight\"] = value.T\n",
    "            # Layer norms - note: your model uses RMSNorm with 'w' parameter\n",
    "            elif \"attn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_1.weight\"] = value\n",
    "            elif \"ffn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_2.weight\"] = value\n",
    "            # FFN weights - include bias terms from your model\n",
    "            elif \"ffn.w_1.weight\" in key:\n",
    "                # Your model: (3072, 768) -> GPT2: (768, 3072)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.weight\"] = value.T\n",
    "            elif \"ffn.w_1.bias\" in key:\n",
    "                # Include bias for c_fc\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.bias\"] = value\n",
    "            elif \"ffn.out.weight\" in key:\n",
    "                # Your model: (768, 3072) -> GPT2: (3072, 768)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.weight\"] = value.T\n",
    "            elif \"ffn.out.bias\" in key:\n",
    "                # Include bias for c_proj\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.bias\"] = value\n",
    "    \n",
    "    # Add missing components that GPT2 expects but your model doesn't have\n",
    "    # 1. Positional embeddings - your model uses RoPE, so initialize to zeros\n",
    "    hf_state_dict[\"transformer.wpe.weight\"] = torch.zeros(1024, 768)\n",
    "    \n",
    "    # 2. Layer norm bias terms - your model uses RMSNorm without bias\n",
    "    hf_state_dict[\"transformer.ln_f.bias\"] = torch.zeros(768)\n",
    "    for layer_num in range(5):  # 5 layers\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_1.bias\"] = torch.zeros(768)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_2.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # 3. Attention bias terms - your model doesn't have these\n",
    "    for layer_num in range(5):  # 5 layers\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.bias\"] = torch.zeros(2304)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # Save as pytorch_model.bin\n",
    "    torch.save(hf_state_dict, output_path)\n",
    "\n",
    "# Convert your checkpoint\n",
    "# convert_checkpoint_to_hf(\"best_val_loss_dense_step_9000.pt\", \"pytorch_model.bin\")\n",
    "# print(\"Conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_converted_model(weight_path, test_input):\n",
    "    \"\"\"Test the converted model by loading it and running a simple generation.\"\"\"\n",
    "    # Test the conversion\n",
    "    from transformers import GPT2LMHeadModel, GPT2Config\n",
    "    import torch\n",
    "\n",
    "    # Load config from your local file\n",
    "    config = GPT2Config.from_pretrained(\"./\")  # Current directory\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Load your converted weights (using the latest converted model)\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "\n",
    "    # print(\"Converted state_dict keys:\")\n",
    "    # for k in sorted(state_dict.keys()):\n",
    "    #     print(f\"  {k}: {state_dict[k].shape}\")\n",
    "\n",
    "    # print(\"\\nExpected model keys:\")\n",
    "    # for k in sorted(model.state_dict().keys()):\n",
    "    #     print(f\"  {k}: {model.state_dict()[k].shape}\")\n",
    "\n",
    "    # Try loading with strict=False first to see what happens\n",
    "    try:\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        print(f\"\\nLoading report:\")\n",
    "        print(f\"Missing keys: {len(missing_keys)}\")\n",
    "        if missing_keys:\n",
    "            print(\"  \", missing_keys[:5], \"...\" if len(missing_keys) > 5 else \"\")\n",
    "            \n",
    "        print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
    "        if unexpected_keys:\n",
    "            print(\"  \", unexpected_keys[:5], \"...\" if len(unexpected_keys) > 5 else \"\")\n",
    "            \n",
    "        if len(missing_keys) == 0 and len(unexpected_keys) == 0:\n",
    "            print(\"✅ Perfect conversion!\")\n",
    "            \n",
    "            # Test the model with a simple generation\n",
    "            model.eval()\n",
    "            from transformers import AutoTokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "            # Test generation\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(inputs[\"input_ids\"], max_length=20, do_sample=True, temperature=0.7)\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\nTest generation:\")\n",
    "            print(f\"Input: {test_input}\")\n",
    "            print(f\"Output: {generated_text}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️  Conversion completed with some mismatches\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during loading: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 checkpoint files in dense/:\n",
      "  - best_val_loss_dense_step_5100.pt\n",
      "  - best_val_loss_dense_step_8400.pt\n",
      "  - best_val_loss_dense_step_300.pt\n",
      "  - best_val_loss_dense_step_3000.pt\n",
      "  - best_val_loss_dense_step_2700.pt\n",
      "  - best_val_loss_dense_step_6600.pt\n",
      "  - dense_step_8000.pt\n",
      "  - best_val_loss_dense_step_3600.pt\n",
      "  - best_val_loss_dense_step_5700.pt\n",
      "  - best_val_loss_dense_step_2100.pt\n",
      "  - dense_step_9000.pt\n",
      "  - best_val_loss_dense_step_6000.pt\n",
      "  - best_val_loss_dense_step_7500.pt\n",
      "  - best_val_loss_dense_step_6300.pt\n",
      "  - best_val_loss_dense_step_1800.pt\n",
      "  - dense_step_5000.pt\n",
      "  - dense_step_7000.pt\n",
      "  - best_val_loss_dense_step_5400.pt\n",
      "  - best_val_loss_dense_step_1500.pt\n",
      "  - best_val_loss_dense_step_4200.pt\n",
      "  - best_val_loss_dense_step_3900.pt\n",
      "  - best_val_loss_dense_step_8100.pt\n",
      "  - best_val_loss_dense_step_7800.pt\n",
      "  - best_val_loss_dense_step_9000.pt\n",
      "  - dense_step_6000.pt\n",
      "  - best_val_loss_dense_step_6900.pt\n",
      "  - best_val_loss_dense_step_4500.pt\n",
      "  - best_val_loss_dense_step_1200.pt\n",
      "  - best_val_loss_dense_step_2400.pt\n",
      "  - best_val_loss_dense_step_4800.pt\n",
      "  - best_val_loss_dense_step_3300.pt\n",
      "  - best_val_loss_dense_step_7200.pt\n",
      "  - last_epoch_dense_0.pt\n",
      "  - best_val_loss_dense_step_8700.pt\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_5100.pt -> dense-converted/best_val_loss_dense_step_5100.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_8400.pt -> dense-converted/best_val_loss_dense_step_8400.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_300.pt -> dense-converted/best_val_loss_dense_step_300.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_3000.pt -> dense-converted/best_val_loss_dense_step_3000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_2700.pt -> dense-converted/best_val_loss_dense_step_2700.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_6600.pt -> dense-converted/best_val_loss_dense_step_6600.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: dense_step_8000.pt -> dense-converted/dense_step_8000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_3600.pt -> dense-converted/best_val_loss_dense_step_3600.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_5700.pt -> dense-converted/best_val_loss_dense_step_5700.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_2100.pt -> dense-converted/best_val_loss_dense_step_2100.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: dense_step_9000.pt -> dense-converted/dense_step_9000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_6000.pt -> dense-converted/best_val_loss_dense_step_6000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_7500.pt -> dense-converted/best_val_loss_dense_step_7500.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_6300.pt -> dense-converted/best_val_loss_dense_step_6300.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_1800.pt -> dense-converted/best_val_loss_dense_step_1800.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: dense_step_5000.pt -> dense-converted/dense_step_5000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: dense_step_7000.pt -> dense-converted/dense_step_7000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_5400.pt -> dense-converted/best_val_loss_dense_step_5400.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_1500.pt -> dense-converted/best_val_loss_dense_step_1500.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_4200.pt -> dense-converted/best_val_loss_dense_step_4200.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_3900.pt -> dense-converted/best_val_loss_dense_step_3900.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_8100.pt -> dense-converted/best_val_loss_dense_step_8100.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_7800.pt -> dense-converted/best_val_loss_dense_step_7800.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_9000.pt -> dense-converted/best_val_loss_dense_step_9000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: dense_step_6000.pt -> dense-converted/dense_step_6000.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_6900.pt -> dense-converted/best_val_loss_dense_step_6900.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_4500.pt -> dense-converted/best_val_loss_dense_step_4500.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_1200.pt -> dense-converted/best_val_loss_dense_step_1200.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_2400.pt -> dense-converted/best_val_loss_dense_step_2400.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_4800.pt -> dense-converted/best_val_loss_dense_step_4800.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_3300.pt -> dense-converted/best_val_loss_dense_step_3300.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_7200.pt -> dense-converted/best_val_loss_dense_step_7200.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: last_epoch_dense_0.pt -> dense-converted/last_epoch_dense_0.bin\n",
      "==================================================\n",
      "conversion complete\n",
      "\n",
      "==================================================\n",
      "Converting: best_val_loss_dense_step_8700.pt -> dense-converted/best_val_loss_dense_step_8700.bin\n",
      "==================================================\n",
      "conversion complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Create output directory for converted models\n",
    "output_dir = \"dense-converted/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all .pt files in the @/dense directory\n",
    "dense_dir = \"dense/\"\n",
    "if os.path.exists(dense_dir):\n",
    "    pt_files = glob.glob(os.path.join(dense_dir, \"*.pt\"))\n",
    "    \n",
    "    if pt_files:\n",
    "        print(f\"Found {len(pt_files)} checkpoint files in {dense_dir}:\")\n",
    "        for pt_file in pt_files:\n",
    "            print(f\"  - {os.path.basename(pt_file)}\")\n",
    "        \n",
    "        # Convert each checkpoint\n",
    "        for pt_file in pt_files:\n",
    "            base_name = os.path.splitext(os.path.basename(pt_file))[0]\n",
    "            output_name = os.path.join(output_dir, f\"{base_name}.bin\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Converting: {os.path.basename(pt_file)} -> {output_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            convert_checkpoint_to_hf(pt_file, output_name)\n",
    "            print(\"conversion complete\")\n",
    "            # test_converted_model(output_name, \"the quick brown fox jumps over the lazy dog\")\n",
    "    else:\n",
    "        print(f\"No .pt files found in {dense_dir}\")\n",
    "else:\n",
    "    print(f\"Directory {dense_dir} does not exist\")\n",
    "    # Fallback to original single file conversion\n",
    "    # convert_checkpoint_to_hf(\"best_val_loss_dense_step_9000.pt\", os.path.join(output_dir, \"best_val_loss_dense_step_9000.bin\"))\n",
    "    # test_converted_model(os.path.join(output_dir, \"best_val_loss_dense_step_9000.bin\"), \"the quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GPT-2 tokenizer files...\n",
      "Saved tokenizer files:\n",
      "  - ./vocab.json\n",
      "  - ./merges.txt\n",
      "  - ./tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save GPT-2 tokenizer files (vocab.json and merges.txt) for HuggingFace upload\n",
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define output directory (adjust as needed)\n",
    "output_dir = \"./\"  # Current directory, or specify your desired path\n",
    "\n",
    "print(\"Saving GPT-2 tokenizer files...\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Method 1: The easy way - let transformers handle it\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Or Method 2: Manual save (if you need more control)\n",
    "# Save vocab.json\n",
    "vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer.get_vocab(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save merges.txt  \n",
    "merges_path = os.path.join(output_dir, \"merges.txt\")\n",
    "with open(merges_path, 'w', encoding='utf-8') as f:\n",
    "    bpe_merges = tokenizer.bpe_ranks\n",
    "    if bpe_merges:\n",
    "        f.write(\"#version: 0.2\\n\")\n",
    "        for merge_tuple in bpe_merges:\n",
    "            f.write(f\"{merge_tuple[0]} {merge_tuple[1]}\\n\")\n",
    "\n",
    "# Save tokenizer_config.json\n",
    "tokenizer_config_path = os.path.join(output_dir, \"tokenizer_config.json\")\n",
    "tokenizer_config = {\n",
    "    \"model_max_length\": 1024,\n",
    "    \"tokenizer_class\": \"GPT2Tokenizer\",\n",
    "    \"bos_token\": \"<|endoftext|>\",\n",
    "    \"eos_token\": \"<|endoftext|>\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"pad_token\": \"<|endoftext|>\"\n",
    "}\n",
    "with open(tokenizer_config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(tokenizer_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved tokenizer files:\")\n",
    "print(f\"  - {vocab_path}\")\n",
    "print(f\"  - {merges_path}\")\n",
    "print(f\"  - {tokenizer_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
