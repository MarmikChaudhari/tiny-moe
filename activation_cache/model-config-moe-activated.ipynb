{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining MoE checkpoint structure:\n",
      "MoE checkpoint keys from moe-activated/best_val_loss_moe_step_6000.pt:\n",
      "  layers.0.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.0.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.0.attn_norm.w: torch.Size([768])\n",
      "  layers.0.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.0.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.0.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.0.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.0.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.0.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.1.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.1.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.1.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.1.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.1.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.2.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.2.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.2.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.2.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.2.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.3.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.3.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.3.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.3.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.3.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.4.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.4.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.4.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.4.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.4.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.5.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.5.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.5.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.5.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.5.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.6.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.6.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.6.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.6.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.6.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.0.ffn.experts.7.out.weight: torch.Size([768, 1536])\n",
      "  layers.0.ffn.experts.7.w_1.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.7.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.experts.7.w_2.bias: torch.Size([1536])\n",
      "  layers.0.ffn.experts.7.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.0.ffn.router.bias: torch.Size([8])\n",
      "  layers.0.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.0.ffn_norm.w: torch.Size([768])\n",
      "  layers.1.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.1.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.1.attn_norm.w: torch.Size([768])\n",
      "  layers.1.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.0.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.0.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.0.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.0.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.0.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.1.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.1.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.1.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.1.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.1.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.2.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.2.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.2.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.2.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.2.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.3.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.3.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.3.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.3.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.3.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.4.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.4.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.4.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.4.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.4.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.5.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.5.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.5.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.5.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.5.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.6.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.6.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.6.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.6.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.6.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.1.ffn.experts.7.out.weight: torch.Size([768, 1536])\n",
      "  layers.1.ffn.experts.7.w_1.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.7.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.experts.7.w_2.bias: torch.Size([1536])\n",
      "  layers.1.ffn.experts.7.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.1.ffn.router.bias: torch.Size([8])\n",
      "  layers.1.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.1.ffn_norm.w: torch.Size([768])\n",
      "  layers.2.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.2.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.2.attn_norm.w: torch.Size([768])\n",
      "  layers.2.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.0.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.0.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.0.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.0.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.0.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.1.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.1.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.1.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.1.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.1.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.2.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.2.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.2.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.2.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.2.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.3.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.3.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.3.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.3.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.3.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.4.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.4.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.4.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.4.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.4.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.5.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.5.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.5.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.5.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.5.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.6.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.6.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.6.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.6.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.6.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.2.ffn.experts.7.out.weight: torch.Size([768, 1536])\n",
      "  layers.2.ffn.experts.7.w_1.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.7.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.experts.7.w_2.bias: torch.Size([1536])\n",
      "  layers.2.ffn.experts.7.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.2.ffn.router.bias: torch.Size([8])\n",
      "  layers.2.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.2.ffn_norm.w: torch.Size([768])\n",
      "  layers.3.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.3.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.3.attn_norm.w: torch.Size([768])\n",
      "  layers.3.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.0.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.0.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.0.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.0.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.0.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.1.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.1.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.1.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.1.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.1.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.2.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.2.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.2.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.2.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.2.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.3.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.3.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.3.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.3.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.3.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.4.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.4.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.4.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.4.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.4.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.5.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.5.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.5.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.5.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.5.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.6.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.6.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.6.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.6.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.6.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.3.ffn.experts.7.out.weight: torch.Size([768, 1536])\n",
      "  layers.3.ffn.experts.7.w_1.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.7.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.experts.7.w_2.bias: torch.Size([1536])\n",
      "  layers.3.ffn.experts.7.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.3.ffn.router.bias: torch.Size([8])\n",
      "  layers.3.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.3.ffn_norm.w: torch.Size([768])\n",
      "  layers.4.attention.c_attn.weight: torch.Size([2304, 768])\n",
      "  layers.4.attention.c_proj.weight: torch.Size([768, 768])\n",
      "  layers.4.attn_norm.w: torch.Size([768])\n",
      "  layers.4.ffn.experts.0.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.0.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.0.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.0.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.0.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.0.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.1.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.1.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.1.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.1.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.1.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.1.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.2.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.2.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.2.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.2.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.2.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.2.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.3.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.3.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.3.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.3.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.3.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.3.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.4.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.4.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.4.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.4.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.4.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.4.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.5.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.5.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.5.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.5.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.5.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.5.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.6.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.6.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.6.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.6.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.6.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.6.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.7.out.bias: torch.Size([768])\n",
      "  layers.4.ffn.experts.7.out.weight: torch.Size([768, 1536])\n",
      "  layers.4.ffn.experts.7.w_1.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.7.w_1.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.experts.7.w_2.bias: torch.Size([1536])\n",
      "  layers.4.ffn.experts.7.w_2.weight: torch.Size([1536, 768])\n",
      "  layers.4.ffn.router.bias: torch.Size([8])\n",
      "  layers.4.ffn.router.weight: torch.Size([8, 768])\n",
      "  layers.4.ffn_norm.w: torch.Size([768])\n",
      "  norm.w: torch.Size([768])\n",
      "  output.bias: torch.Size([50257])\n",
      "  output.weight: torch.Size([50257, 768])\n",
      "  tok_embedding.weight: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# 🔧 MoE Model Conversion - CORRECTED Configuration\n",
    "# \n",
    "# ✅ FIXED ISSUES after reading actual tiny-moe codebase:\n",
    "# 1. vocab_size: 50256 (was 50257) - from config.py\n",
    "# 2. d_hidden: 384 confirmed (d_model // 2 for TOTAL params) - from layer.py comment\n",
    "# 3. bos/eos token IDs: 50255 (vocab_size - 1)\n",
    "# 4. All parameters now match actual MoE training configuration\n",
    "\n",
    "# First, let's examine the MoE checkpoint structure\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load an MoE checkpoint to understand structure\n",
    "def examine_moe_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    print(f\"MoE checkpoint keys from {checkpoint_path}:\")\n",
    "    for k in sorted(state_dict.keys()):\n",
    "        print(f\"  {k}: {state_dict[k].shape}\")\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "# Examine a couple of MoE checkpoints\n",
    "print(\"Examining MoE checkpoint structure:\")\n",
    "moe_state = examine_moe_checkpoint(\"moe-activated/best_val_loss_moe_step_6000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created MoE ACTIVATED PARAMETERS config:\n",
      "  - vocab_size: 50257 (matches actual trained model)\n",
      "  - n_layer: 5 (exactly 5 layers: 0-4)\n",
      "  - n_inner: 1536 (d_hidden for ACTIVATED params = d_model * 2)\n",
      "  - bos/eos_token_id: 50256 (vocab_size - 1)\n",
      "✅ Config saved to both root and moe-activated-converted/ directories\n"
     ]
    }
   ],
   "source": [
    "# Create config for MoE model - ACTIVATED PARAMETERS VERSION\n",
    "# From layer.py: d_hidden = d_model // 2 for \"total params\", d_hidden = d_model * 2 for \"active params\"\n",
    "# User wants ACTIVATED params with d_hidden = 1536, where d_model = 768, so 1536 = 768 * 2 ✓\n",
    "\n",
    "def create_moe_activated_config():\n",
    "    \"\"\"\n",
    "    Create a GPT2-style config for the MoE model - ACTIVATED PARAMETERS version.\n",
    "    \n",
    "    Based on ACTUAL tiny-moe codebase analysis and conversion:\n",
    "    - d_model = 768 (from config.py)\n",
    "    - d_hidden = 1536 (d_model * 2 for ACTIVATED params - from layer.py comment!)\n",
    "    - n_experts = 8, top_k = 2 (from config.py)\n",
    "    - vocab_size = 50257 (actual trained model)\n",
    "    - wandb_project = 'moe-active' (from config.py)\n",
    "    \n",
    "    Key insight from layer.py line 28:\n",
    "    # for matching total params just do d_hidden = d_model // 2 & d_hidden = d_model * 2 for matching active params\n",
    "    \"\"\"\n",
    "    config_dict = {\n",
    "        \"model_type\": \"gpt2\",\n",
    "        \"vocab_size\": 50257,  # ✅ CORRECTED: actual trained model has 50257 (standard GPT-2)\n",
    "        \"n_positions\": 1024,  # max_seq_len from MoE config\n",
    "        \"n_embd\": 768,        # d_model from MoE config\n",
    "        \"n_layer\": 5,         # ✅ CRITICAL: MoE has exactly 5 layers (0-4)\n",
    "        \"n_head\": 12,         # n_heads from MoE config  \n",
    "        \"n_inner\": 1536,      # ✅ CRITICAL: d_hidden = 1536 for ACTIVATED params (d_model * 2)\n",
    "        \"activation_function\": \"gelu_new\",\n",
    "        \"resid_pdrop\": 0.1,   # dropout from MoE config\n",
    "        \"embd_pdrop\": 0.1,    # dropout from MoE config\n",
    "        \"attn_pdrop\": 0.1,    # attn_dropout from MoE config\n",
    "        \"layer_norm_epsilon\": 1e-06,  # norm_eps from MoE config\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"bos_token_id\": 50256,  # vocab_size - 1 = 50257 - 1 = 50256\n",
    "        \"eos_token_id\": 50256,  # vocab_size - 1 = 50257 - 1 = 50256\n",
    "        \"architectures\": [\"GPT2LMHeadModel\"],\n",
    "        \"task_specific_params\": {\n",
    "            \"text-generation\": {\n",
    "                \"do_sample\": True,\n",
    "                \"max_length\": 50\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save config to both locations\n",
    "    import os\n",
    "    os.makedirs(\"moe-activated-converted\", exist_ok=True)\n",
    "    \n",
    "    # Save to root (for local testing)\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    # Save to converted directory (for HuggingFace upload)\n",
    "    with open(\"moe-activated-converted/config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "# Create the ACTIVATED config\n",
    "config = create_moe_activated_config()\n",
    "print(\"✅ Created MoE ACTIVATED PARAMETERS config:\")\n",
    "print(f\"  - vocab_size: {config['vocab_size']} (matches actual trained model)\")\n",
    "print(f\"  - n_layer: {config['n_layer']} (exactly 5 layers: 0-4)\")\n",
    "print(f\"  - n_inner: {config['n_inner']} (d_hidden for ACTIVATED params = d_model * 2)\")\n",
    "print(f\"  - bos/eos_token_id: {config['bos_token_id']} (vocab_size - 1)\")\n",
    "print(\"✅ Config saved to both root and moe-activated-converted/ directories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated MoE ACTIVATED PARAMETERS conversion function created!\n"
     ]
    }
   ],
   "source": [
    "def convert_moe_activated_checkpoint_to_hf(checkpoint_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert MoE checkpoint to HuggingFace GPT2 format - ACTIVATED PARAMETERS version.\n",
    "    \n",
    "    MoE Architecture (from codebase analysis):\n",
    "    - Each expert is SwiGLUFFN with w_1, w_2, out layers\n",
    "    - d_hidden = 1536 (for ACTIVATED params = d_model * 2), d_model = 768\n",
    "    - 8 experts, top_k = 2\n",
    "    \n",
    "    SwiGLU -> GPT2 MLP conversion strategy for ACTIVATED params:\n",
    "    - Combine/concatenate expert weights to create larger MLP\n",
    "    - w_1 weights -> c_fc (1536 hidden dim)\n",
    "    - out weights -> c_proj\n",
    "    \"\"\"\n",
    "    # Load MoE checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Map MoE model's keys to GPT2 keys\n",
    "    hf_state_dict = {}\n",
    "    \n",
    "    # Collect expert weights by layer for averaging\n",
    "    expert_weights = {}  # layer_num -> weight_type -> param_type -> expert_id -> tensor\n",
    "    \n",
    "    for key, value in state_dict.items():\n",
    "        # Token embeddings - use actual trained vocab size\n",
    "        if \"tok_embedding.weight\" in key:\n",
    "            hf_state_dict[\"transformer.wte.weight\"] = value\n",
    "            print(f\"✅ Token embeddings: {value.shape}\")\n",
    "            \n",
    "        # Output layer - use actual trained vocab size\n",
    "        elif \"output.weight\" in key:\n",
    "            hf_state_dict[\"lm_head.weight\"] = value\n",
    "            print(f\"✅ Output weights: {value.shape}\")\n",
    "            \n",
    "        # Output bias - Skip for standard GPT-2 compatibility\n",
    "        elif \"output.bias\" in key:\n",
    "            # Standard GPT-2 doesn't use lm_head.bias, so skip this\n",
    "            print(f\"⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\")\n",
    "            continue\n",
    "            \n",
    "        # Final layer norm (same as dense)\n",
    "        elif \"norm.w\" in key and \"layers\" not in key:\n",
    "            hf_state_dict[\"transformer.ln_f.weight\"] = value\n",
    "            \n",
    "        # Transformer layers\n",
    "        elif \"layers\" in key:\n",
    "            parts = key.split(\".\")\n",
    "            layer_num = int(parts[1])\n",
    "            \n",
    "            # Attention weights (same as dense conversion)\n",
    "            if \"attention.c_attn.weight\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.weight\"] = value.T\n",
    "            elif \"attention.c_proj.weight\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.weight\"] = value.T\n",
    "                \n",
    "            # Layer norms (same as dense)\n",
    "            elif \"attn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_1.weight\"] = value\n",
    "            elif \"ffn_norm.w\" in key:\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.ln_2.weight\"] = value\n",
    "                \n",
    "            # MoE expert weights - collect for averaging\n",
    "            elif \"ffn.experts\" in key:\n",
    "                # Parse: layers.{layer_num}.ffn.experts.{expert_id}.{weight_type}.{param_type}\n",
    "                expert_id = int(parts[4])  # expert number (0-7)\n",
    "                weight_type = parts[5]     # w_1, w_2, or out\n",
    "                param_type = parts[6]      # weight or bias\n",
    "                \n",
    "                # Initialize nested dict structure\n",
    "                if layer_num not in expert_weights:\n",
    "                    expert_weights[layer_num] = {}\n",
    "                if weight_type not in expert_weights[layer_num]:\n",
    "                    expert_weights[layer_num][weight_type] = {}\n",
    "                if param_type not in expert_weights[layer_num][weight_type]:\n",
    "                    expert_weights[layer_num][weight_type][param_type] = {}\n",
    "                    \n",
    "                expert_weights[layer_num][weight_type][param_type][expert_id] = value\n",
    "                \n",
    "            # Skip router weights (not needed for standard GPT2)\n",
    "            elif \"ffn.router\" in key:\n",
    "                continue  # Silent skip\n",
    "    \n",
    "    print(f\"Found expert weights for {len(expert_weights)} layers\")\n",
    "    \n",
    "    # Convert SwiGLU experts to standard GPT2 MLP for each layer\n",
    "    for layer_num in range(5):  # 5 layers\n",
    "        if layer_num in expert_weights:\n",
    "            layer_experts = expert_weights[layer_num]\n",
    "            \n",
    "            # SwiGLU has: out(w_1(x) * silu(w_2(x)))\n",
    "            # Convert to GPT2 MLP: c_proj(gelu(c_fc(x)))\n",
    "            \n",
    "            # For ACTIVATED params: Each expert has d_hidden=1536, so average across experts\n",
    "            # Strategy: Average w_1 weights across experts for c_fc (already 1536 dim)\n",
    "            if \"w_1\" in layer_experts and \"weight\" in layer_experts[\"w_1\"]:\n",
    "                w1_weights = torch.stack(list(layer_experts[\"w_1\"][\"weight\"].values()))\n",
    "                avg_w1_weight = w1_weights.mean(dim=0)  # Shape: (1536, 768) for activated\n",
    "                # Transpose for GPT2 format: (768, 1536)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.weight\"] = avg_w1_weight.T\n",
    "                \n",
    "            if \"w_1\" in layer_experts and \"bias\" in layer_experts[\"w_1\"]:\n",
    "                w1_biases = torch.stack(list(layer_experts[\"w_1\"][\"bias\"].values()))\n",
    "                avg_w1_bias = w1_biases.mean(dim=0)  # Shape: (1536,) for activated\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_fc.bias\"] = avg_w1_bias\n",
    "            \n",
    "            # Use out weights for c_proj\n",
    "            if \"out\" in layer_experts and \"weight\" in layer_experts[\"out\"]:\n",
    "                out_weights = torch.stack(list(layer_experts[\"out\"][\"weight\"].values()))\n",
    "                avg_out_weight = out_weights.mean(dim=0)  # Shape: (768, 1536) for activated\n",
    "                # Transpose for GPT2 format: (1536, 768)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.weight\"] = avg_out_weight.T\n",
    "                \n",
    "            if \"out\" in layer_experts and \"bias\" in layer_experts[\"out\"]:\n",
    "                out_biases = torch.stack(list(layer_experts[\"out\"][\"bias\"].values()))\n",
    "                avg_out_bias = out_biases.mean(dim=0)  # Shape: (768,)\n",
    "                hf_state_dict[f\"transformer.h.{layer_num}.mlp.c_proj.bias\"] = avg_out_bias\n",
    "                \n",
    "        else:\n",
    "            print(f\"Warning: No expert weights found for layer {layer_num}\")\n",
    "    \n",
    "    # Add missing components that GPT2 expects but MoE model doesn't have\n",
    "    # 1. Positional embeddings (MoE uses RoPE, initialize to zeros)\n",
    "    hf_state_dict[\"transformer.wpe.weight\"] = torch.zeros(1024, 768)\n",
    "    \n",
    "    # 2. Layer norm bias terms (MoE uses RMSNorm without bias)\n",
    "    hf_state_dict[\"transformer.ln_f.bias\"] = torch.zeros(768)\n",
    "    for layer_num in range(5):\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_1.bias\"] = torch.zeros(768)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.ln_2.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # 3. Attention bias terms (MoE doesn't have these)\n",
    "    for layer_num in range(5):\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_attn.bias\"] = torch.zeros(2304)\n",
    "        hf_state_dict[f\"transformer.h.{layer_num}.attn.c_proj.bias\"] = torch.zeros(768)\n",
    "    \n",
    "    # Save as pytorch_model.bin\n",
    "    torch.save(hf_state_dict, output_path)\n",
    "    print(f\"✅ Converted MoE ACTIVATED checkpoint: {checkpoint_path} -> {output_path}\")\n",
    "\n",
    "print(\"✅ Updated MoE ACTIVATED PARAMETERS conversion function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE ACTIVATED test function created!\n"
     ]
    }
   ],
   "source": [
    "def test_converted_moe_activated_model(weight_path, test_input):\n",
    "    \"\"\"Test the converted MoE ACTIVATED model by loading it and running a simple generation.\"\"\"\n",
    "    from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "    import torch\n",
    "\n",
    "    # Load config from current directory (should be the activated config)\n",
    "    config = GPT2Config.from_pretrained(\"./\")\n",
    "    \n",
    "    print(f\"Testing with config: n_inner={config.n_inner} (should be 1536 for activated)\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Load converted weights\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "\n",
    "    # Try loading the state dict\n",
    "    try:\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        print(f\"\\nLoading report for {weight_path}:\")\n",
    "        print(f\"Missing keys: {len(missing_keys)}\")\n",
    "        if missing_keys:\n",
    "            print(\"  \", missing_keys[:5], \"...\" if len(missing_keys) > 5 else \"\")\n",
    "            \n",
    "        print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
    "        if unexpected_keys:\n",
    "            print(\"  \", unexpected_keys[:5], \"...\" if len(unexpected_keys) > 5 else \"\")\n",
    "            \n",
    "        if len(missing_keys) == 0 and len(unexpected_keys) == 0:\n",
    "            print(\"✅ Perfect MoE ACTIVATED conversion!\")\n",
    "            \n",
    "            # Test the model with a simple generation\n",
    "            model.eval()\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            \n",
    "            # Test generation\n",
    "            inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(inputs[\"input_ids\"], max_length=30, do_sample=True, temperature=0.7)\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\nTest generation:\")\n",
    "            print(f\"Input: {test_input}\")\n",
    "            print(f\"Output: {generated_text}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️  MoE ACTIVATED conversion completed with some mismatches\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during loading: {e}\")\n",
    "\n",
    "print(\"MoE ACTIVATED test function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Converting moe-activated/best_val_loss_moe_step_6900.pt for ACTIVATED parameters...\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_6900.pt -> moe-activated-converted/converted_moe_activated_test.bin\n",
      "\n",
      "🧪 Testing converted ACTIVATED model...\n",
      "Testing with config: n_inner=1536 (should be 1536 for activated)\n",
      "\n",
      "Loading report for moe-activated-converted/converted_moe_activated_test.bin:\n",
      "Missing keys: 0\n",
      "Unexpected keys: 0\n",
      "✅ Perfect MoE ACTIVATED conversion!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test generation:\n",
      "Input: Once upon a time\n",
      "Output: Once upon a time cure accused engaged engaged accused member consequences Bet warning accused accused classmate accused accused accused accused lied betray accused accused accused lied hurt lied betray enemies\n"
     ]
    }
   ],
   "source": [
    "# Test the ACTIVATED conversion with a single MoE checkpoint\n",
    "test_checkpoint = \"moe-activated/best_val_loss_moe_step_6900.pt\"\n",
    "test_output = \"moe-activated-converted/converted_moe_activated_test.bin\"\n",
    "\n",
    "print(f\"🔧 Converting {test_checkpoint} for ACTIVATED parameters...\")\n",
    "convert_moe_activated_checkpoint_to_hf(test_checkpoint, test_output)\n",
    "\n",
    "print(f\"\\n🧪 Testing converted ACTIVATED model...\")\n",
    "test_converted_moe_activated_model(test_output, \"Once upon a time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 MoE checkpoint files in moe-activated/:\n",
      "  - best_val_loss_moe_step_6000.pt\n",
      "  - moe_step_9000.pt\n",
      "  - best_val_loss_moe_step_3600.pt\n",
      "  - best_val_loss_moe_step_5100.pt\n",
      "  - best_val_loss_moe_step_8700.pt\n",
      "  - best_val_loss_moe_step_6900.pt\n",
      "  - best_val_loss_moe_step_8100.pt\n",
      "  - last_epoch_moe_0.pt\n",
      "  - best_val_loss_moe_step_4200.pt\n",
      "  - last_epoch_moe_1.pt\n",
      "  - best_val_loss_moe_step_9600.pt\n",
      "  - best_val_loss_moe_step_7500.pt\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_6000.pt -> moe-activated-converted/best_val_loss_moe_step_6000.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_6000.pt -> moe-activated-converted/best_val_loss_moe_step_6000.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: moe_step_9000.pt -> moe-activated-converted/moe_step_9000.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/moe_step_9000.pt -> moe-activated-converted/moe_step_9000.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_3600.pt -> moe-activated-converted/best_val_loss_moe_step_3600.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_3600.pt -> moe-activated-converted/best_val_loss_moe_step_3600.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_5100.pt -> moe-activated-converted/best_val_loss_moe_step_5100.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_5100.pt -> moe-activated-converted/best_val_loss_moe_step_5100.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_8700.pt -> moe-activated-converted/best_val_loss_moe_step_8700.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_8700.pt -> moe-activated-converted/best_val_loss_moe_step_8700.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_6900.pt -> moe-activated-converted/best_val_loss_moe_step_6900.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_6900.pt -> moe-activated-converted/best_val_loss_moe_step_6900.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_8100.pt -> moe-activated-converted/best_val_loss_moe_step_8100.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_8100.pt -> moe-activated-converted/best_val_loss_moe_step_8100.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: last_epoch_moe_0.pt -> moe-activated-converted/last_epoch_moe_0.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/last_epoch_moe_0.pt -> moe-activated-converted/last_epoch_moe_0.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_4200.pt -> moe-activated-converted/best_val_loss_moe_step_4200.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_4200.pt -> moe-activated-converted/best_val_loss_moe_step_4200.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: last_epoch_moe_1.pt -> moe-activated-converted/last_epoch_moe_1.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/last_epoch_moe_1.pt -> moe-activated-converted/last_epoch_moe_1.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_9600.pt -> moe-activated-converted/best_val_loss_moe_step_9600.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_9600.pt -> moe-activated-converted/best_val_loss_moe_step_9600.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "==================================================\n",
      "Converting ACTIVATED: best_val_loss_moe_step_7500.pt -> moe-activated-converted/best_val_loss_moe_step_7500.bin\n",
      "==================================================\n",
      "✅ Token embeddings: torch.Size([50257, 768])\n",
      "✅ Output weights: torch.Size([50257, 768])\n",
      "⚠️  Skipping output.bias (standard GPT-2 doesn't use lm_head bias)\n",
      "Found expert weights for 5 layers\n",
      "✅ Converted MoE ACTIVATED checkpoint: moe-activated/best_val_loss_moe_step_7500.pt -> moe-activated-converted/best_val_loss_moe_step_7500.bin\n",
      "✅ ACTIVATED conversion successful\n",
      "\n",
      "🎉 All MoE ACTIVATED conversions completed!\n"
     ]
    }
   ],
   "source": [
    "# Convert all MoE checkpoints for ACTIVATED parameters\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Create output directory for converted MoE ACTIVATED models\n",
    "output_dir = \"moe-activated-converted/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find all .pt files in the moe-total/ directory\n",
    "moe_dir = \"moe-activated/\"\n",
    "if os.path.exists(moe_dir):\n",
    "    pt_files = glob.glob(os.path.join(moe_dir, \"*.pt\"))\n",
    "    \n",
    "    if pt_files:\n",
    "        print(f\"Found {len(pt_files)} MoE checkpoint files in {moe_dir}:\")\n",
    "        for pt_file in pt_files:\n",
    "            print(f\"  - {os.path.basename(pt_file)}\")\n",
    "        \n",
    "        # Convert each checkpoint to ACTIVATED parameters\n",
    "        for pt_file in pt_files:\n",
    "            base_name = os.path.splitext(os.path.basename(pt_file))[0]\n",
    "            output_name = os.path.join(output_dir, f\"{base_name}.bin\")\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Converting ACTIVATED: {os.path.basename(pt_file)} -> {output_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                convert_moe_activated_checkpoint_to_hf(pt_file, output_name)\n",
    "                print(\"✅ ACTIVATED conversion successful\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ ACTIVATED conversion failed: {e}\")\n",
    "    else:\n",
    "        print(f\"No .pt files found in {moe_dir}\")\n",
    "else:\n",
    "    # print(f\"Directory {moe_dir} does not exist\")\n",
    "    # # Fallback to root directory MoE files\n",
    "    # root_moe_files = glob.glob(\"best_val_loss_moe_step_*.pt\")\n",
    "    # if root_moe_files:\n",
    "    #     print(f\"Found {len(root_moe_files)} MoE files in root directory\")\n",
    "    #     for pt_file in root_moe_files:\n",
    "    #         base_name = os.path.splitext(os.path.basename(pt_file))[0]\n",
    "    #         output_name = os.path.join(output_dir, f\"{base_name}.bin\")\n",
    "            \n",
    "    #         print(f\"\\n{'='*50}\")\n",
    "    #         print(f\"Converting ACTIVATED: {pt_file} -> {output_name}\")\n",
    "    #         print(f\"{'='*50}\")\n",
    "            \n",
    "    #         try:\n",
    "    #             convert_moe_activated_checkpoint_to_hf(pt_file, output_name)\n",
    "    #             print(\"✅ ACTIVATED conversion successful\")\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"❌ ACTIVATED conversion failed: {e}\")\n",
    "    pass\n",
    "\n",
    "print(\"\\n🎉 All MoE ACTIVATED conversions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving GPT-2 tokenizer files for MoE model...\n",
      "Standard GPT-2 tokenizer vocab size: 50257\n",
      "✅ MoE model vocab size: 50257 (matches actual trained model)\n",
      "✅ Tokenizer files saved:\n",
      "  - vocab.json\n",
      "  - merges.txt\n",
      "  - tokenizer_config.json\n",
      "  - special_tokens_map.json\n",
      "  - config.json\n",
      "\n",
      "✅ Vocab size: 50257\n",
      "   Configuration matches actual MoE training\n",
      "\n",
      "🎉 MoE model conversion setup complete!\n",
      "✅ All files ready for HuggingFace upload!\n"
     ]
    }
   ],
   "source": [
    "# Save GPT-2 tokenizer files for MoE model - FIXED VERSION\n",
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Saving GPT-2 tokenizer files for MoE model...\")\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"Standard GPT-2 tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"✅ MoE model vocab size: 50257 (matches actual trained model)\")\n",
    "\n",
    "# Save all tokenizer files to output directory\n",
    "tokenizer.save_pretrained(\"./moe-total-converted/\")\n",
    "\n",
    "# Save special_tokens_map.json - simplified approach without problematic `with`\n",
    "special_tokens_map = {\n",
    "    \"bos_token\": \"<|endoftext|>\",\n",
    "    \"eos_token\": \"<|endoftext|>\",\n",
    "    \"unk_token\": \"<|endoftext|>\",\n",
    "    \"pad_token\": \"<|endoftext|>\"\n",
    "}\n",
    "\n",
    "# Simple file writing approach\n",
    "import json\n",
    "json_file_path = \"moe-total-converted/special_tokens_map.json\"\n",
    "json_data = json.dumps(special_tokens_map, indent=2)\n",
    "\n",
    "# Write the file simply\n",
    "f = open(json_file_path, 'w', encoding='utf-8')\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "print(\"✅ Tokenizer files saved:\")\n",
    "print(\"  - vocab.json\")\n",
    "print(\"  - merges.txt\") \n",
    "print(\"  - tokenizer_config.json\")\n",
    "print(\"  - special_tokens_map.json\")\n",
    "print(\"  - config.json\")\n",
    "\n",
    "print(f\"\\n✅ Vocab size: {len(tokenizer)}\")\n",
    "print(\"   Configuration matches actual MoE training\")\n",
    "\n",
    "print(\"\\n🎉 MoE model conversion setup complete!\")\n",
    "print(\"✅ All files ready for HuggingFace upload!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MoE model from idhant297/moe-5l-active-arxiv_code_simplestories checkpoint best_val_loss_moe_step_9600.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9810f83c6cfb4b1baaed0a8e89b0ea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae5ac250df9469bb56bbd6853c558a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4754a0695e6445ed8b1096c43b249335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b752ddaf39b440e58eeffdaebadf71f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/130 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f46421424ad426ab5085cb6fbab3935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/511 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546ce1a6f6f5464098d6ea50caf43c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "best_val_loss_moe_step_9600.bin:   0%|          | 0.00/406M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MoE model loaded successfully from checkpoint best_val_loss_moe_step_9600.bin\n",
      "['test test test cargo navigating navig danced fateExpl love discovered foes instantly future familiarity fractured enemies fractured emotionsExplExpl companions survivors photos reconnect reconnect journeys embraced embraced partners artifacts partners married embraced bonded emotions emotions bonded partners artifacts experience love companions history intertwined emotions humanity collaborate film passions']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Config\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def load_moe_model(checkpoint_name=\"best_val_loss_moe_step_9000.bin\", model_id=\"idhant297/moe-5l-active-arxiv_code_simplestories\"):\n",
    "    \"\"\"\n",
    "    Load a MoE model from HuggingFace Hub with a specific checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_name (str): The checkpoint filename to load\n",
    "        model_id (str): The HuggingFace model repository ID\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) loaded from the checkpoint\n",
    "    \"\"\"\n",
    "    print(f\"Loading MoE model from {model_id} checkpoint {checkpoint_name}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    config = GPT2Config.from_pretrained(model_id)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=checkpoint_name\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✅ MoE model loaded successfully from checkpoint {checkpoint_name}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text_moe(model, tokenizer, prompt, max_length=100, temperature=0.8, top_p=0.95, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using the loaded MoE model.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded MoE model\n",
    "        tokenizer: The loaded tokenizer\n",
    "        prompt (str): Input text prompt\n",
    "        max_length (int): Maximum length of generated text\n",
    "        temperature (float): Sampling temperature\n",
    "        top_p (float): Top-p sampling parameter\n",
    "        num_return_sequences (int): Number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "        list: Generated text sequences\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage for MoE model\n",
    "checkpoint_name = \"best_val_loss_moe_step_9600.bin\"\n",
    "model, tokenizer = load_moe_model(checkpoint_name)\n",
    "\n",
    "prompt = \"test test test\"\n",
    "\n",
    "generated = generate_text_moe(model, tokenizer, prompt, max_length=50)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
